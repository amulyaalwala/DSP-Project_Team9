{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [224, 224]\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing VGG with its own weights,\n",
    "# include_top false means it removes top dense layers which are optimized for VGG images \n",
    "# we need to add new layers and train with our data.\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False) # here [3] is 3 channels Red blue Green\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "  layer.trainable = False\n",
    "#Here we Froze the weights of VGG- Meaning we are using existing features from VGG adn mixing with our dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model without any dense layers, features only from VGNET and direct predictions.\n",
    "x = Flatten()(vgg.output)\n",
    "# x = Dense(1000, activation='relu')(x)\n",
    "prediction = Dense(5, activation='softmax')(x) # here we have 5 output classes so i gave 5 in output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating VGNET model\n",
    "model = Model(inputs=vgg.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 5)                 125445    \n",
      "=================================================================\n",
      "Total params: 14,840,133\n",
      "Trainable params: 125,445\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer=tf.keras.optimizers.RMSprop(),\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 214 images belonging to 5 classes.\n",
      "Found 81 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#Input needs to be (224,224)\n",
    "# we are addign data augmentation to zoom in out , and flipping horizontally\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)# add augments with horizontal flip\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)#divide by 255 to normalize/scale\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('SecondDataset/Train',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('SecondDataset/Test',\n",
    "                                           target_size = (224, 224),\n",
    "                                             batch_size = 32,\n",
    "                                             class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 8s 1s/step - loss: 4.9197 - accuracy: 0.2336 - val_loss: 2.8664 - val_accuracy: 0.3827\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 7s 929ms/step - loss: 2.0409 - accuracy: 0.4533 - val_loss: 0.7690 - val_accuracy: 0.7037\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 7s 932ms/step - loss: 1.5301 - accuracy: 0.4579 - val_loss: 1.4758 - val_accuracy: 0.4691\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 7s 953ms/step - loss: 1.2293 - accuracy: 0.5935 - val_loss: 0.9201 - val_accuracy: 0.5802\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 7s 969ms/step - loss: 1.0365 - accuracy: 0.6776 - val_loss: 0.6704 - val_accuracy: 0.7407\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=5,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 492ms/step - loss: 0.6704 - accuracy: 0.7407\n",
      "Test Loss: 0.6704394817352295\n",
      "Test accuracy: 0.7407407760620117\n"
     ]
    }
   ],
   "source": [
    "#This did not work very will with VG net architecture.\n",
    "#Transfer learning may not always work. we need to Tweak some parameters and try it agian,\n",
    "score = model.evaluate(test_set)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i am adding more train layers to customize the model more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our layers - you can add more if you want\n",
    "x = Flatten()(vgg.output)\n",
    "# x = Dense(1000, activation='relu')(x)\n",
    "x=Dense(100, activation='relu')(x)\n",
    "Dropout(0.5)(x)\n",
    "x=Dense(100, activation='relu')(x)\n",
    "prediction = Dense(5, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model object\n",
    "model = Model(inputs=vgg.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               2508900   \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 17,234,193\n",
      "Trainable params: 2,519,505\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer=tf.keras.optimizers.RMSprop(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "callback = [earlystop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "7/7 [==============================] - 7s 978ms/step - loss: 5.6932 - accuracy: 0.2009 - val_loss: 1.4766 - val_accuracy: 0.4321\n",
      "Epoch 2/8\n",
      "7/7 [==============================] - 6s 892ms/step - loss: 1.9636 - accuracy: 0.2757 - val_loss: 1.4225 - val_accuracy: 0.4074\n",
      "Epoch 3/8\n",
      "7/7 [==============================] - 6s 892ms/step - loss: 1.3922 - accuracy: 0.4252 - val_loss: 1.4853 - val_accuracy: 0.6173\n",
      "Epoch 4/8\n",
      "7/7 [==============================] - 6s 871ms/step - loss: 1.3472 - accuracy: 0.4953 - val_loss: 1.9951 - val_accuracy: 0.2346\n",
      "Epoch 5/8\n",
      "7/7 [==============================] - 6s 875ms/step - loss: 1.2555 - accuracy: 0.4533 - val_loss: 1.7040 - val_accuracy: 0.3827\n",
      "Epoch 6/8\n",
      "7/7 [==============================] - 6s 872ms/step - loss: 1.2323 - accuracy: 0.5234 - val_loss: 0.9181 - val_accuracy: 0.5679\n",
      "Epoch 7/8\n",
      "7/7 [==============================] - 6s 845ms/step - loss: 0.8553 - accuracy: 0.6729 - val_loss: 0.7792 - val_accuracy: 0.7901\n",
      "Epoch 8/8\n",
      "7/7 [==============================] - 6s 851ms/step - loss: 0.9802 - accuracy: 0.6075 - val_loss: 0.7195 - val_accuracy: 0.7160\n"
     ]
    }
   ],
   "source": [
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=8,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set),callbacks=callback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding more layers which are trainablle did not help our model.We need to try a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 448ms/step - loss: 0.7195 - accuracy: 0.7160\n",
      "Test Loss: 0.7194870710372925\n",
      "Test accuracy: 0.7160493731498718\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_set)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thrid Model - Now i will only use one dense layer and one for output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "for layer in vgg.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our layers - you can add more if you want\n",
    "x = Flatten()(vgg.output)\n",
    "# x = Dense(1000, activation='relu')(x)\n",
    "x=Dense(50, activation='relu')(x)\n",
    "prediction = Dense(5, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer=tf.keras.optimizers.RMSprop(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "callback = [earlystop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "7/7 [==============================] - 7s 990ms/step - loss: 1.7249 - accuracy: 0.5421 - val_loss: 1.1603 - val_accuracy: 0.5556\n",
      "Epoch 2/8\n",
      "7/7 [==============================] - 6s 883ms/step - loss: 1.1930 - accuracy: 0.5561 - val_loss: 0.4561 - val_accuracy: 0.8765\n",
      "Epoch 3/8\n",
      "7/7 [==============================] - 6s 852ms/step - loss: 0.6041 - accuracy: 0.7523 - val_loss: 0.8231 - val_accuracy: 0.5926\n",
      "Epoch 4/8\n",
      "7/7 [==============================] - 6s 846ms/step - loss: 0.6869 - accuracy: 0.6822 - val_loss: 1.0543 - val_accuracy: 0.6173\n",
      "Epoch 5/8\n",
      "7/7 [==============================] - 6s 848ms/step - loss: 0.6117 - accuracy: 0.7570 - val_loss: 0.2377 - val_accuracy: 0.9506\n",
      "Epoch 6/8\n",
      "7/7 [==============================] - 6s 871ms/step - loss: 0.5032 - accuracy: 0.7850 - val_loss: 0.3464 - val_accuracy: 0.8642\n",
      "Epoch 7/8\n",
      "7/7 [==============================] - 6s 880ms/step - loss: 0.6883 - accuracy: 0.7850 - val_loss: 0.3869 - val_accuracy: 0.8148\n",
      "Epoch 8/8\n",
      "7/7 [==============================] - 7s 909ms/step - loss: 0.4351 - accuracy: 0.8178 - val_loss: 0.2019 - val_accuracy: 0.9630\n"
     ]
    }
   ],
   "source": [
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=8,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set),callbacks=callback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 467ms/step - loss: 0.2019 - accuracy: 0.9630\n",
      "Test Loss: 0.20189321041107178\n",
      "Test accuracy: 0.9629629850387573\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_set)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model Fitted Best compared to the model whit direct predictions of our data with VGG \n",
    "#it worked better than the one with 2 dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its good to use simple models than complex models with more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we explored the Power of transfer learning, by using existing features from VGG\n",
    "# and adding only one dense layer other than output we saved a lot of compute power and time and\n",
    "#achieved great accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
